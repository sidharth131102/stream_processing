# ------------------------------------------------------------------------------
# CLOUD COMPOSER CONTROL PLANE CONFIGURATION
# ------------------------------------------------------------------------------
# This file defines EVERYTHING related to orchestration:
# - Which GCP project Composer runs in
# - How the Composer environment is created
# - Which service account runs Airflow DAGs
# - IAM permissions required by DAGs
# - Airflow variables consumed by pipelines
# - How DAGs are deployed into Composer
#
# IMPORTANT:
# This file is uploaded to the SAME config bucket as pipeline.yaml
# and is read dynamically during Composer setup.
# ------------------------------------------------------------------------------

# ------------------------------------------------------------------------------
# PROJECT IDENTIFICATION
# ------------------------------------------------------------------------------
# Identifies the GCP project where the Composer environment will live.
# These values MUST match the project used in pipeline.yaml.
project:
  id: stream-accelerator-3          # GCP Project ID (REPLACE per environment)
  number: 1091663053251          # GCP Project Number (DO NOT GUESS – fetch from GCP)

# ------------------------------------------------------------------------------
# COMPOSER ENVIRONMENT DEFINITION
# ------------------------------------------------------------------------------
composer:
  environment:
    name: data-platform-dev-c3      # Composer environment name (unique per project/region)
    region: us-east1                # Composer region (can differ from Dataflow region)

    # Cloud Composer v3 image
    # Ties the Airflow version and Python runtime.
    # Upgrade ONLY after validating DAG compatibility.
    image_version: composer-3-airflow-2.10.5

    # Environment sizing:
    # small  → dev / low traffic
    # medium → staging
    # large  → production
    environment_size: small

    # Optional labels (highly recommended for platform teams)
    # Used for cost tracking, ownership, and governance.
    labels:
      platform: stream-accelerator-3
      env: dev
      owner: data-platform

  # ----------------------------------------------------------------------------
  # COMPOSER SERVICE ACCOUNT
  # ----------------------------------------------------------------------------
  # This service account executes ALL Airflow DAG tasks.
  # It must have permissions to:
  # - Start/stop Dataflow jobs
  # - Read/write Pub/Sub
  # - Read/write GCS
  # - Create and manage BigQuery jobs
  service_account:
    name: composer-sa               # Service account name (auto-created)
    display_name: Cloud Composer Service Account

    # IAM roles required for DAG execution
    # WARNING:
    # Removing roles may break DAGs at runtime.
    roles:
      - roles/composer.worker       # Required for Airflow task execution
      - roles/dataflow.admin        # Required to create/stop Dataflow jobs
      - roles/dataflow.worker       # Required for Dataflow interaction
      - roles/pubsub.subscriber     # Required to read from Pub/Sub
      - roles/pubsub.publisher      # Required to publish to Pub/Sub (DLQ)
      - roles/pubsub.viewer         # Required for Pub/Sub metadata
      - roles/storage.objectAdmin   # Required to manage GCS objects
      - roles/bigquery.jobUser      # Required to run BigQuery jobs
      - roles/bigquery.dataViewer   # Required to read BigQuery tables
      - roles/bigquery.dataEditor   # Required to write BigQuery tables
      - roles/iam.serviceAccountUser # Required to impersonate Dataflow SAs

  # ----------------------------------------------------------------------------
  # COMPOSER SERVICE AGENT (REQUIRED FOR COMPOSER v3)
  # ----------------------------------------------------------------------------
  # This is a Google-managed service agent.
  # DO NOT MODIFY unless the project number changes.
  service_agent:
    email: service-1091663053251@cloudcomposer-accounts.iam.gserviceaccount.com
    roles:
      - roles/composer.ServiceAgentV2Ext

  # ----------------------------------------------------------------------------
  # AIRFLOW RUNTIME CONFIGURATION
  # ----------------------------------------------------------------------------
  airflow:
    variables:
      # These Airflow variables are read by ALL DAGs.
      # They allow DAGs to dynamically load pipeline configuration
      # without hardcoding any paths.
      config_bucket: stream-accelerator-3-config   # GCS bucket holding config YAMLs
      pipeline_yaml_path: pipeline.yaml             # Entry-point pipeline config file
      bq_location: us-central1                      # BigQuery dataset location

    # Optional Airflow configuration overrides.
    # Safe defaults are used here.
    config_overrides:
      core.load_examples: "False"       # Disable example DAGs
      webserver.expose_config: "True"   # Allows viewing config in UI (dev-friendly)

  # ----------------------------------------------------------------------------
  # DAG DEPLOYMENT STRATEGY
  # ----------------------------------------------------------------------------
  dags:
    # Local path in the repository where DAG files live.
    local_path: dags/

    # DAG deployment strategy:
    # rsync     → syncs DAGs and deletes removed ones (RECOMMENDED)
    # overwrite → copies files without cleanup
    sync_strategy: rsync

    # Path inside the Composer GCS bucket where DAGs are stored.
    remote_path: dags/

  # ----------------------------------------------------------------------------
  # VALIDATION & SAFETY CHECKS
  # ----------------------------------------------------------------------------
  validation:
    fail_if_env_exists: false        # If true, setup fails when env already exists
    verify_bucket_owner: true        # Ensures config bucket belongs to this project
    verify_airflow_variables: true   # Ensures required Airflow vars are present
